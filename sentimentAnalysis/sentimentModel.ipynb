{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import joblib as jb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['text', 'sentiment']]\n",
    "test_df = test_df[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(train_df[(train_df['sentiment']=='neutral')].index, inplace = True)\n",
    "test_df.drop(test_df[(test_df['sentiment']=='neutral')].index, inplace = True)\n",
    "\n",
    "train_df = train_df[train_df['text'].notna()]\n",
    "test_df = test_df[test_df['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_all_punct(text):\n",
    "    # print(text)\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# Remove numbers, replace it by NUMBER\n",
    "def remove_number(text):\n",
    "    num = re.compile(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*')\n",
    "    return num.sub(r'NUMBER', text)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# text_preprocess :\n",
    "# Makes text lower, removes all punctuation, removes number and replaces\n",
    "# it with string \"NUMBER\", tokenizes the text and then removes stop words.\n",
    "# -----------------------------------------------------------------------\n",
    "def text_preprocess(text):\n",
    "    # porter = PorterStemmer()\n",
    "    text = remove_all_punct(text)\n",
    "    text = remove_number(text)\n",
    "    text = text.lower()\n",
    "    # text = porter.stem(text)\n",
    "    return text\n",
    "\n",
    "def text_stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = ' '.join(stemmer.stem(token) for token in word_tokenize(text))\n",
    "    return text\n",
    "\n",
    "def text_tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "def text_tokenize_with_stopwords(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['preprocessed_text'] = train_df['text'].apply(lambda x : text_preprocess(x))\n",
    "test_df['preprocessed_text'] = test_df['text'].apply(lambda x : text_preprocess(x))\n",
    "                                                    \n",
    "train_df['stemmed_text'] = train_df[\"preprocessed_text\"].apply(lambda x : text_stemmer(x))\n",
    "test_df['stemmed_text'] = test_df[\"preprocessed_text\"].apply(lambda x : text_stemmer(x))\n",
    "\n",
    "train_df['tokens'] = train_df['preprocessed_text'].apply(lambda x : text_tokenize(x))\n",
    "test_df['tokens'] = test_df['preprocessed_text'].apply(lambda x : text_tokenize(x))\n",
    "\n",
    "train_df['tokens_with_sw'] = train_df['preprocessed_text'].apply(lambda x : text_tokenize_with_stopwords(x))\n",
    "test_df['tokens_with_sw'] = test_df['preprocessed_text'].apply(lambda x : text_tokenize_with_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder =LabelEncoder()\n",
    "train_df['sentiment_encoded']= label_encoder.fit_transform(train_df['sentiment'])\n",
    "test_df['sentiment_encoded']= label_encoder.fit_transform(test_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "MODEL_FILENAME = 'sentiment_model.pkl'\n",
    "VECT_FILENAME = 'sentiment_vectorizer.pkl'\n",
    "def count_vector(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit(data.values.astype('U'))\n",
    "    vect = count_vectorizer.transform(data.values.astype('U'))\n",
    "    jb.dump(count_vectorizer, VECT_FILENAME)\n",
    "    return vect, count_vectorizer\n",
    "\n",
    "def tfidf_vector(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    vect = tfidf_vectorizer.fit_transform(data.values.astype('U'))\n",
    "    return vect, tfidf_vectorizer\n",
    "\n",
    "X_train_count, count_vectorizer = count_vector(train_df[\"stemmed_text\"])\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf_vector(train_df[\"stemmed_text\"])\n",
    "\n",
    "X_test_count = count_vectorizer.transform(test_df[\"stemmed_text\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df[\"stemmed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_state = 42\n",
    "\n",
    "models=[\n",
    "        # XGBClassifier(max_depth=6, n_estimators=500),\n",
    "        # SVC(random_state=random_state, kernel='linear'),\n",
    "        LogisticRegression(solver = 'sag', random_state=random_state),\n",
    "        # RandomForestClassifier(n_estimators=500,random_state=random_state),\n",
    "        # MultinomialNB(),\n",
    "        # DecisionTreeClassifier(random_state = random_state),\n",
    "        # KNeighborsClassifier(),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = []\n",
    "# CV = 5\n",
    "# cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "# entries = []\n",
    "def fit_and_predict(model,x_train,x_test,y_train,y_test,vectorizer):\n",
    "    classifier = model\n",
    "    classifier_name = str(classifier.__class__.__name__)\n",
    "    eval_set = [(x_test, y_test)]\n",
    "    classifier.fit(x_train,y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    if(classifier_name=='LogisticRegression' and str(vectorizer)=='Count vector'):\n",
    "        jb.dump(classifier, MODEL_FILENAME)\n",
    "    # accuracies  = cross_val_score(model, x, y, scoring='accuracy', cv=CV)\n",
    "    # for fold_idx, accuracy in enumerate(accuracies):\n",
    "    #         entries.append((str(classifier.__class__.__name__),str(vectorizer), fold_idx, accuracy))\n",
    "\n",
    "    f1score = f1_score(y_test,y_pred,average='weighted')\n",
    "    train_accuracy = round(classifier.score(x_train,y_train)*100)\n",
    "    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)\n",
    "    \n",
    "    \n",
    "    metric.append({\n",
    "        \"model\": classifier_name,\n",
    "        \"f1 score\": f1score, \n",
    "        \"train accuracy\": train_accuracy, \n",
    "        \"test accuracy\": test_accuracy, \n",
    "        \"vectorizer\": str(vectorizer),\n",
    "        })\n",
    "\n",
    "    print(str(classifier.__class__.__name__) +\" using \"+ str(vectorizer))\n",
    "    print(classification_report(y_test,y_pred))    \n",
    "    # print('Accuracy over splitted train and test set')\n",
    "    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n",
    "    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     y_train = train_df.sentiment_encoded\n",
    "#     y_test = test_df.sentiment_encoded\n",
    "#     # x = X_train_count\n",
    "#     # x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)\n",
    "#     fit_and_predict(model,X_train_count,X_test_count,y_train,y_test,'Count vector')\n",
    "    \n",
    "#     # x = X_train_tfidf\n",
    "#     # # x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)\n",
    "#     # fit_and_predict(model,X_train_tfidf,X_test_tfidf,y_train,y_test, 'Tfidf vector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_df = pd.DataFrame(metric)\n",
    "# metric_df = metric_df.sort_values('f1 score', ascending=False)\n",
    "# metric_df.to_csv('model_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = '/Applications/Github/Negotiation-Bot/sentimentAnalysis/sentiment_model.pkl'\n",
    "VECT_FILENAME = '/Users/kabirbatra/Downloads/sentimentAnalysis/sentiment_vectorizer.pkl'\n",
    "def load_model_and_vectorizer():\n",
    "    # if os.path.exists(MODEL_FILENAME):\n",
    "    model = jb.load(MODEL_FILENAME)\n",
    "    vect = jb.load(VECT_FILENAME)\n",
    "    return model, vect\n",
    "\n",
    "model, vect = load_model_and_vectorizer()\n",
    "def predict_intent(text):\n",
    "    preprocessed_text = text_preprocess(text)\n",
    "    vectorized_text = vect.transform([preprocessed_text])\n",
    "    # predicting the intent\n",
    "    label = model.predict(vectorized_text)\n",
    "    print(label_encoder.classes_[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_intent = predict_intent(\"very old product\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
