{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "df = pd.read_csv('./train_data.csv')\n",
    "labels = df['intent'].unique()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kabirbatra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#labeling integer values to classes\n",
    "df['intent'].replace({'init-price':0, 'insist':1, 'counter-price':2, 'agree':3,\n",
    "       'intro':4, 'inquiry':5, 'disagree':6, 'vague-price':7, 'inform':8, 'unknown':9}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "#Creating vocabulary list for each class \n",
    "voc_list0 = []\n",
    "def add_0(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list0.append(i)\n",
    "\n",
    "df[df['intent'] == 0]['tokens'].apply(lambda x: add_0(x))\n",
    "\n",
    "voc_list1 = []\n",
    "def add_1(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list1.append(i)\n",
    "\n",
    "df[df['intent'] == 1]['tokens'].apply(lambda x: add_1(x))\n",
    "\n",
    "voc_list2 = []\n",
    "def add_2(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list2.append(i)\n",
    "\n",
    "df[df['intent'] == 2]['tokens'].apply(lambda x: add_2(x))\n",
    "\n",
    "voc_list3 = []\n",
    "def add_3(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list3.append(i)\n",
    "\n",
    "df[df['intent'] == 3]['tokens'].apply(lambda x: add_3(x))\n",
    "\n",
    "voc_list4 = []\n",
    "def add_4(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list4.append(i)\n",
    "\n",
    "df[df['intent'] == 4]['tokens'].apply(lambda x: add_4(x))\n",
    "\n",
    "voc_list5 = []\n",
    "def add_5(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list5.append(i)\n",
    "\n",
    "df[df['intent'] == 5]['tokens'].apply(lambda x: add_5(x))\n",
    "\n",
    "voc_list6 = []\n",
    "def add_6(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list6.append(i)\n",
    "\n",
    "df[df['intent'] == 6]['tokens'].apply(lambda x: add_6(x))\n",
    "\n",
    "voc_list7 = []\n",
    "def add_7(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list7.append(i)\n",
    "\n",
    "df[df['intent'] == 7]['tokens'].apply(lambda x: add_7(x))\n",
    "\n",
    "voc_list8 = []\n",
    "def add_8(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list8.append(i)\n",
    "\n",
    "df[df['intent'] == 8]['tokens'].apply(lambda x: add_8(x))\n",
    "\n",
    "voc_list9 = []\n",
    "def add_9(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list9.append(i)\n",
    "\n",
    "df[df['intent'] == 9]['tokens'].apply(lambda x: add_9(x))\n",
    "\n",
    "#variable declaration for stemming and lemmatization \n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "#grouping words, taking count for each word and correcting misspells \n",
    "voc_list0 = pd.DataFrame(voc_list0, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list0.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list0.reset_index(inplace=True)\n",
    "voc_list0['words'] = [ps.stem(x) for x in voc_list0['words']]\n",
    "voc_list0['words'] = [lm.lemmatize(x,pos='v') for x in voc_list0['words']]\n",
    "voc_list0['words'] = voc_list0['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list0 = voc_list0.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list1 = pd.DataFrame(voc_list1, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list1.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list1.reset_index(inplace=True)\n",
    "voc_list1['words'] = [ps.stem(x) for x in voc_list1['words']]\n",
    "voc_list1['words'] = [lm.lemmatize(x,pos='v') for x in voc_list1['words']]\n",
    "voc_list1['words'] = voc_list1['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list1 = voc_list1.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list2 = pd.DataFrame(voc_list2, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list2.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list2.reset_index(inplace=True)\n",
    "voc_list2['words'] = [ps.stem(x) for x in voc_list2['words']]\n",
    "voc_list2['words'] = [lm.lemmatize(x,pos='v') for x in voc_list2['words']]\n",
    "voc_list2['words'] = voc_list2['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list2 = voc_list2.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list3 = pd.DataFrame(voc_list3, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list3.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list3.reset_index(inplace=True)\n",
    "voc_list3['words'] = [ps.stem(x) for x in voc_list3['words']]\n",
    "voc_list3['words'] = [lm.lemmatize(x,pos='v') for x in voc_list3['words']]\n",
    "voc_list3['words'] = voc_list3['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list3 = voc_list3.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list4 = pd.DataFrame(voc_list4, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list4.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list4.reset_index(inplace=True)\n",
    "voc_list4['words'] = [ps.stem(x) for x in voc_list4['words']]\n",
    "voc_list4['words'] = [lm.lemmatize(x,pos='v') for x in voc_list4['words']]\n",
    "voc_list4['words'] = voc_list4['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list4 = voc_list4.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list5 = pd.DataFrame(voc_list5, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list5.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list5.reset_index(inplace=True)\n",
    "voc_list5['words'] = [ps.stem(x) for x in voc_list5['words']]\n",
    "voc_list5['words'] = [lm.lemmatize(x,pos='v') for x in voc_list5['words']]\n",
    "voc_list5['words'] = voc_list5['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list5 = voc_list5.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list6 = pd.DataFrame(voc_list7, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list6.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list6.reset_index(inplace=True)\n",
    "voc_list6['words'] = [ps.stem(x) for x in voc_list6['words']]\n",
    "voc_list6['words'] = [lm.lemmatize(x,pos='v') for x in voc_list6['words']]\n",
    "voc_list6['words'] = voc_list6['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list6 = voc_list6.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list7 = pd.DataFrame(voc_list7, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list7.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list7.reset_index(inplace=True)\n",
    "voc_list7['words'] = [ps.stem(x) for x in voc_list7['words']]\n",
    "voc_list7['words'] = [lm.lemmatize(x,pos='v') for x in voc_list7['words']]\n",
    "voc_list7['words'] = voc_list7['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list7 = voc_list7.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list8 = pd.DataFrame(voc_list8, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list8.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list8.reset_index(inplace=True)\n",
    "voc_list8['words'] = [ps.stem(x) for x in voc_list8['words']]\n",
    "voc_list8['words'] = [lm.lemmatize(x,pos='v') for x in voc_list8['words']]\n",
    "voc_list8['words'] = voc_list8['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list8 = voc_list8.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list9 = pd.DataFrame(voc_list9, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list9.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list9.reset_index(inplace=True)\n",
    "voc_list9['words'] = [ps.stem(x) for x in voc_list9['words']]\n",
    "voc_list9['words'] = [lm.lemmatize(x,pos='v') for x in voc_list9['words']]\n",
    "voc_list9['words'] = voc_list9['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list9 = voc_list9.groupby('words').agg({'count': np.sum})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "voc_list0.reset_index(inplace=True)\n",
    "voc_list1.reset_index(inplace=True)\n",
    "voc_list2.reset_index(inplace=True)\n",
    "voc_list3.reset_index(inplace=True)\n",
    "voc_list4.reset_index(inplace=True)\n",
    "voc_list5.reset_index(inplace=True)\n",
    "voc_list6.reset_index(inplace=True)\n",
    "voc_list7.reset_index(inplace=True)\n",
    "voc_list8.reset_index(inplace=True)\n",
    "voc_list9.reset_index(inplace=True)\n",
    "\n",
    "#Exporting DataFrames to csv files\n",
    "voc_list0[['words','count']].to_csv('vocabulary_list0.csv', header=False, index=False)\n",
    "voc_list1[['words','count']].to_csv('vocabulary_list1.csv', header=False, index=False)\n",
    "voc_list2[['words','count']].to_csv('vocabulary_list2.csv', header=False, index=False)\n",
    "voc_list3[['words','count']].to_csv('vocabulary_list3.csv', header=False, index=False)\n",
    "voc_list4[['words','count']].to_csv('vocabulary_list4.csv', header=False, index=False)\n",
    "voc_list5[['words','count']].to_csv('vocabulary_list5.csv', header=False, index=False)\n",
    "voc_list6[['words','count']].to_csv('vocabulary_list6.csv', header=False, index=False)\n",
    "voc_list7[['words','count']].to_csv('vocabulary_list7.csv', header=False, index=False)\n",
    "voc_list8[['words','count']].to_csv('vocabulary_list8.csv', header=False, index=False)\n",
    "voc_list9[['words','count']].to_csv('vocabulary_list9.csv', header=False, index=False)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot insert level_0, already exists",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-7d70e1d098ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvoc_list0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# voc_list1.reset_index(inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# voc_list2.reset_index(inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# voc_list3.reset_index(inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# voc_list4.reset_index(inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[1;32m   5014\u001b[0m                 \u001b[0;31m# to ndarray and maybe infer different dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5015\u001b[0m                 \u001b[0mlevel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_casted_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5016\u001b[0;31m                 \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5018\u001b[0m         \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_duplicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {item}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert level_0, already exists"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}