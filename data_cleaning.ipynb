{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "df = pd.read_csv('./train_data.csv')\n",
    "labels = df['intent'].unique()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kabirbatra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#labeling integer values to classes\n",
    "df['intent'].replace({'init-price':0, 'insist':1, 'counter-price':2, 'agree':3,\n",
    "       'intro':4, 'inquiry':5, 'disagree':6, 'vague-price':7, 'inform':8, 'unknown':9}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "#Creating vocabulary list for each class \n",
    "voc_list0 = []\n",
    "def add_0(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list0.append(i)\n",
    "\n",
    "df[df['intent'] == 0]['tokens'].apply(lambda x: add_0(x))\n",
    "\n",
    "voc_list1 = []\n",
    "def add_1(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list1.append(i)\n",
    "\n",
    "df[df['intent'] == 1]['tokens'].apply(lambda x: add_1(x))\n",
    "\n",
    "voc_list2 = []\n",
    "def add_2(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list2.append(i)\n",
    "\n",
    "df[df['intent'] == 2]['tokens'].apply(lambda x: add_2(x))\n",
    "\n",
    "voc_list3 = []\n",
    "def add_3(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list3.append(i)\n",
    "\n",
    "df[df['intent'] == 3]['tokens'].apply(lambda x: add_3(x))\n",
    "\n",
    "voc_list4 = []\n",
    "def add_4(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list4.append(i)\n",
    "\n",
    "df[df['intent'] == 4]['tokens'].apply(lambda x: add_4(x))\n",
    "\n",
    "voc_list5 = []\n",
    "def add_5(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list5.append(i)\n",
    "\n",
    "df[df['intent'] == 5]['tokens'].apply(lambda x: add_5(x))\n",
    "\n",
    "voc_list6 = []\n",
    "def add_6(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list6.append(i)\n",
    "\n",
    "df[df['intent'] == 6]['tokens'].apply(lambda x: add_6(x))\n",
    "\n",
    "voc_list7 = []\n",
    "def add_7(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list7.append(i)\n",
    "\n",
    "df[df['intent'] == 7]['tokens'].apply(lambda x: add_7(x))\n",
    "\n",
    "voc_list8 = []\n",
    "def add_8(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list8.append(i)\n",
    "\n",
    "df[df['intent'] == 8]['tokens'].apply(lambda x: add_8(x))\n",
    "\n",
    "voc_list9 = []\n",
    "def add_9(x):\n",
    "    for i in re.findall(r'[\\w]+',x):\n",
    "        voc_list9.append(i)\n",
    "\n",
    "df[df['intent'] == 9]['tokens'].apply(lambda x: add_9(x))\n",
    "\n",
    "#variable declaration for stemming and lemmatization \n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "#grouping words, taking count for each word and correcting misspells \n",
    "voc_list0 = pd.DataFrame(voc_list0, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list0.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list0.reset_index(inplace=True)\n",
    "voc_list0['words'] = [ps.stem(x) for x in voc_list0['words']]\n",
    "voc_list0['words'] = [lm.lemmatize(x,pos='v') for x in voc_list0['words']]\n",
    "voc_list0['words'] = voc_list0['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list0 = voc_list0.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list1 = pd.DataFrame(voc_list1, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list1.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list1.reset_index(inplace=True)\n",
    "voc_list1['words'] = [ps.stem(x) for x in voc_list1['words']]\n",
    "voc_list1['words'] = [lm.lemmatize(x,pos='v') for x in voc_list1['words']]\n",
    "voc_list1['words'] = voc_list1['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list1 = voc_list1.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list2 = pd.DataFrame(voc_list2, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list2.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list2.reset_index(inplace=True)\n",
    "voc_list2['words'] = [ps.stem(x) for x in voc_list2['words']]\n",
    "voc_list2['words'] = [lm.lemmatize(x,pos='v') for x in voc_list2['words']]\n",
    "voc_list2['words'] = voc_list2['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list2 = voc_list2.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list3 = pd.DataFrame(voc_list3, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list3.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list3.reset_index(inplace=True)\n",
    "voc_list3['words'] = [ps.stem(x) for x in voc_list3['words']]\n",
    "voc_list3['words'] = [lm.lemmatize(x,pos='v') for x in voc_list3['words']]\n",
    "voc_list3['words'] = voc_list3['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list3 = voc_list3.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list4 = pd.DataFrame(voc_list4, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list4.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list4.reset_index(inplace=True)\n",
    "voc_list4['words'] = [ps.stem(x) for x in voc_list4['words']]\n",
    "voc_list4['words'] = [lm.lemmatize(x,pos='v') for x in voc_list4['words']]\n",
    "voc_list4['words'] = voc_list4['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list4 = voc_list4.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list5 = pd.DataFrame(voc_list5, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list5.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list5.reset_index(inplace=True)\n",
    "voc_list5['words'] = [ps.stem(x) for x in voc_list5['words']]\n",
    "voc_list5['words'] = [lm.lemmatize(x,pos='v') for x in voc_list5['words']]\n",
    "voc_list5['words'] = voc_list5['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list5 = voc_list5.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list6 = pd.DataFrame(voc_list7, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list6.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list6.reset_index(inplace=True)\n",
    "voc_list6['words'] = [ps.stem(x) for x in voc_list6['words']]\n",
    "voc_list6['words'] = [lm.lemmatize(x,pos='v') for x in voc_list6['words']]\n",
    "voc_list6['words'] = voc_list6['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list6 = voc_list6.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list7 = pd.DataFrame(voc_list7, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list7.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list7.reset_index(inplace=True)\n",
    "voc_list7['words'] = [ps.stem(x) for x in voc_list7['words']]\n",
    "voc_list7['words'] = [lm.lemmatize(x,pos='v') for x in voc_list7['words']]\n",
    "voc_list7['words'] = voc_list7['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list7 = voc_list7.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list8 = pd.DataFrame(voc_list8, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list8.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list8.reset_index(inplace=True)\n",
    "voc_list8['words'] = [ps.stem(x) for x in voc_list8['words']]\n",
    "voc_list8['words'] = [lm.lemmatize(x,pos='v') for x in voc_list8['words']]\n",
    "voc_list8['words'] = voc_list8['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list8 = voc_list8.groupby('words').agg({'count': np.sum})\n",
    "\n",
    "voc_list9 = pd.DataFrame(voc_list9, columns={'words'}).groupby('words').agg({'words': np.size})\n",
    "voc_list9.rename(columns={'words':'count'},inplace=True)\n",
    "voc_list9.reset_index(inplace=True)\n",
    "voc_list9['words'] = [ps.stem(x) for x in voc_list9['words']]\n",
    "voc_list9['words'] = [lm.lemmatize(x,pos='v') for x in voc_list9['words']]\n",
    "voc_list9['words'] = voc_list9['words'].apply(lambda x: TextBlob(x).correct())\n",
    "voc_list9 = voc_list9.groupby('words').agg({'count': np.sum})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "voc_list0.reset_index(inplace=True)\n",
    "voc_list1.reset_index(inplace=True)\n",
    "voc_list2.reset_index(inplace=True)\n",
    "voc_list3.reset_index(inplace=True)\n",
    "voc_list4.reset_index(inplace=True)\n",
    "voc_list5.reset_index(inplace=True)\n",
    "voc_list6.reset_index(inplace=True)\n",
    "voc_list7.reset_index(inplace=True)\n",
    "voc_list8.reset_index(inplace=True)\n",
    "voc_list9.reset_index(inplace=True)\n",
    "\n",
    "#Exporting DataFrames to csv files\n",
    "voc_list0[['words','count']].to_csv('vocabulary_list0.csv', header=False, index=False)\n",
    "voc_list1[['words','count']].to_csv('vocabulary_list1.csv', header=False, index=False)\n",
    "voc_list2[['words','count']].to_csv('vocabulary_list2.csv', header=False, index=False)\n",
    "voc_list3[['words','count']].to_csv('vocabulary_list3.csv', header=False, index=False)\n",
    "voc_list4[['words','count']].to_csv('vocabulary_list4.csv', header=False, index=False)\n",
    "voc_list5[['words','count']].to_csv('vocabulary_list5.csv', header=False, index=False)\n",
    "voc_list6[['words','count']].to_csv('vocabulary_list6.csv', header=False, index=False)\n",
    "voc_list7[['words','count']].to_csv('vocabulary_list7.csv', header=False, index=False)\n",
    "voc_list8[['words','count']].to_csv('vocabulary_list8.csv', header=False, index=False)\n",
    "voc_list9[['words','count']].to_csv('vocabulary_list9.csv', header=False, index=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}